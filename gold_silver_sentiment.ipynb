{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8b19a4",
   "metadata": {},
   "source": [
    "# Mehmet Kubilay Gulacdi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bdb6a2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef717c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\K\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20beac7a",
   "metadata": {},
   "source": [
    "## Read txt and define token identifiers\n",
    "We need to define \"gold\", \"golden\" and \"silver\" word(token) that will be our identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a058f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"sample.txt\", \"r\", encoding=\"utf8\")\n",
    "documents = file.read()\n",
    "\n",
    "gold_tokens = ['gold', 'golden']\n",
    "silver_tokens = ['silver']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86512808",
   "metadata": {},
   "source": [
    "## Sentence and word tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8c849",
   "metadata": {},
   "source": [
    "- Firstly, we need to split the whole documents with using sent_tokenize().\n",
    "- Secondly, we use our sentence variable and use word_tokenize to obtain tokens of every sentence in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f123bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(documents)\n",
    "tokens = [word_tokenize(sentence.lower()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcffbff",
   "metadata": {},
   "source": [
    "- So now we have tokens of every sentence in the documents. \n",
    "- Basically, we need to count our identifiers in our tokens. If a sentence contains identifier, then it must be counted.\n",
    "- Finally, we print our gold_count and silver_count variables to see number of documents containing gold and silver reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc68dac",
   "metadata": {},
   "source": [
    "## Counting number of documents(sentences) containing gold&silver reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51abae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents containing gold reference: 609\n",
      "Number of documents containing silver reference: 116\n"
     ]
    }
   ],
   "source": [
    "gold_count = 0\n",
    "silver_count = 0\n",
    "\n",
    "for token_list in tokens:\n",
    "    if any(token in gold_tokens for token in token_list):\n",
    "        gold_count += 1\n",
    "        \n",
    "    if any(token in silver_tokens for token in token_list):\n",
    "        silver_count += 1\n",
    "\n",
    "print(\"Number of documents containing gold reference:\", gold_count)\n",
    "print(\"Number of documents containing silver reference:\", silver_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a81c0c",
   "metadata": {},
   "source": [
    "- We will calculate the sentiment of sentences and print out overall.\n",
    "- We have some preprocessing part that cleans our text in order to increase sentiment scores.\n",
    "- doc means a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2529621",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa7996",
   "metadata": {},
   "source": [
    "- We define some functions that helps me to remove unneccessary parts of text which are punctuations, numbers, stopwords and emoji. These are typical preprocesses for NLP.\n",
    "- we set these functions to use consecutively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8359b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(doc):\n",
    "  text = doc.split() # basic tokenizer \n",
    "  text = [w for w in doc if w not in string.punctuation] # if words in a sentence(doc) contains punctuation, then we will remove.\n",
    "  return \"\".join(text)\n",
    "\n",
    "def remove_numbers(doc):\n",
    "  text = [w for w in doc if not w.isdigit()] # removing numbers which is unneccessary\n",
    "  return \"\".join(text)\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "  text = [w for w in doc.split() if w not in stopwords.words(\"english\")] # removing stopwords\n",
    "  return \" \".join(text) \n",
    "\n",
    "def remove_emoji(text):\n",
    "    return re.sub(\n",
    "        r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U0001FB00-\\U0001FBFF\\U0001FE00-\\U0001FE0F\\U0001F004]+',\n",
    "        '', text) # removing emoji using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f257715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can easily manipulate text data with using tabular format such as dataframe so we create a dataframe\n",
    "\n",
    "df = pd.DataFrame(columns=['Sentence'])\n",
    "\n",
    "for sentence in sentences:\n",
    "    df = df.append({'Sentence': sentence}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06522bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With apply-lambda duo, we can reach every row (sentence) and we can clean them with our defined functions.\n",
    "# First we do lowercase and then we remove punctuations and then we remove numbers and finally we remove stopwords.\n",
    "# Finally, we create a new column in our dataframe called Clean Sentence in order to continue with this column for sentiment\n",
    "\n",
    "df[\"Clean_Sentence\"] = df[\"Sentence\"].apply(lambda doc: remove_stopwords\n",
    "                                            (remove_numbers\n",
    "                                             (remove_punctuation\n",
    "                                              (remove_emoji\n",
    "                                               (doc.lower())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a1450b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Clean_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@fansoniclove Gold the Tenrec\\nTokyo-bound Sam...</td>\n",
       "      <td>fansoniclove gold tenrec tokyobound sampson se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shooter Dane Sampson has struck career-best fo...</td>\n",
       "      <td>shooter dane sampson struck careerbest form bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sampson registered a score of 462 points to cl...</td>\n",
       "      <td>sampson registered score points claim gold thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The performance bettered Sampson's own nationa...</td>\n",
       "      <td>performance bettered sampsons national record ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The score was also notably higher than what It...</td>\n",
       "      <td>score also notably higher italys niccolo campr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  @fansoniclove Gold the Tenrec\\nTokyo-bound Sam...   \n",
       "1  Shooter Dane Sampson has struck career-best fo...   \n",
       "2  Sampson registered a score of 462 points to cl...   \n",
       "3  The performance bettered Sampson's own nationa...   \n",
       "4  The score was also notably higher than what It...   \n",
       "\n",
       "                                      Clean_Sentence  \n",
       "0  fansoniclove gold tenrec tokyobound sampson se...  \n",
       "1  shooter dane sampson struck careerbest form bu...  \n",
       "2  sampson registered score points claim gold thr...  \n",
       "3  performance bettered sampsons national record ...  \n",
       "4  score also notably higher italys niccolo campr...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5be8ee19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5847, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "# we need number of row because for \"for\" loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c13cd3",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeba5e7",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69f76fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentiment value for gold: 0.19689226973684212\n",
      "Overall sentiment value for silver: 0.23296896551724136\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "gold_sentiments = []\n",
    "silver_sentiments = []\n",
    "clean_sentences = df[\"Clean_Sentence\"] # we define a variable for readability\n",
    "\n",
    "for index in range(0, 5847):\n",
    "    # we reach sentences from 0 to 5847th index sentence respectively and we tokenized them and hold as \"token_list\"\n",
    "    token_list = word_tokenize(clean_sentences.loc[index]) \n",
    "    \n",
    "    if any(token in gold_tokens for token in token_list):  \n",
    "    # if token_list has gold_tokens(identifier) then we calculate polarity scores and append our gold_sentiments list\n",
    "        gold_sentiments.append(analyzer.polarity_scores(' '.join(token_list))['compound'])\n",
    "        \n",
    "    if any(token in silver_tokens for token in token_list):\n",
    "    # same process for silver sentiments\n",
    "        silver_sentiments.append(analyzer.polarity_scores(' '.join(token_list))['compound'])\n",
    "        \n",
    "# We calculate overall sentiment value for gold and silver\n",
    "print(\"Overall sentiment value for gold:\", sum(gold_sentiments) / len(gold_sentiments) if gold_sentiments else 0)\n",
    "print(\"Overall sentiment value for silver:\", sum(silver_sentiments) / len(silver_sentiments) if silver_sentiments else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0acf258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.8555,\n",
       " 0.296,\n",
       " 0.4404,\n",
       " -0.296,\n",
       " 0.34,\n",
       " 0.2023,\n",
       " -0.2023,\n",
       " 0.3612,\n",
       " -0.6124,\n",
       " 0.0,\n",
       " 0.926,\n",
       " 0.5574,\n",
       " 0.4767,\n",
       " 0.875,\n",
       " 0.8807,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.539,\n",
       " -0.6486,\n",
       " -0.128,\n",
       " -0.0516,\n",
       " 0.3182,\n",
       " 0.5423,\n",
       " 0.2263,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4588,\n",
       " 0.8481,\n",
       " 0.8316,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6597,\n",
       " 0.3818,\n",
       " 0.0,\n",
       " -0.1027,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8807,\n",
       " 0.9403,\n",
       " 0.8689,\n",
       " 0.0,\n",
       " 0.7096,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6249,\n",
       " 0.0,\n",
       " 0.7351,\n",
       " 0.4019,\n",
       " 0.128,\n",
       " 0.4019,\n",
       " 0.9001,\n",
       " 0.4404,\n",
       " 0.0,\n",
       " 0.802,\n",
       " 0.0,\n",
       " -0.7783,\n",
       " -0.0516,\n",
       " 0.0,\n",
       " 0.3818,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1531,\n",
       " 0.4404,\n",
       " 0.4588,\n",
       " -0.0258,\n",
       " 0.0,\n",
       " 0.6486,\n",
       " 0.0,\n",
       " -0.5574,\n",
       " 0.3612,\n",
       " 0.5994,\n",
       " 0.5267,\n",
       " 0.1263,\n",
       " 0.4215,\n",
       " -0.2732,\n",
       " 0.0,\n",
       " -0.0516,\n",
       " 0.2732,\n",
       " 0.0,\n",
       " -0.4019,\n",
       " 0.6705,\n",
       " -0.296,\n",
       " -0.4019,\n",
       " 0.0935,\n",
       " 0.7506,\n",
       " 0.8402,\n",
       " 0.0,\n",
       " -0.6808,\n",
       " 0.9081,\n",
       " -0.4767,\n",
       " 0.0,\n",
       " 0.886,\n",
       " -0.1027,\n",
       " 0.0,\n",
       " 0.4767,\n",
       " 0.891,\n",
       " 0.9313,\n",
       " 0.8555,\n",
       " 0.4588,\n",
       " 0.0,\n",
       " 0.2732,\n",
       " 0.8225,\n",
       " 0.8591,\n",
       " 0.0,\n",
       " -0.6597,\n",
       " -0.3818,\n",
       " -0.4767,\n",
       " 0.4404,\n",
       " 0.0,\n",
       " 0.2023,\n",
       " 0.4939,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.128,\n",
       " 0.0,\n",
       " 0.6249,\n",
       " 0.6486,\n",
       " 0.5994,\n",
       " -0.0516,\n",
       " 0.6249,\n",
       " -0.2263,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2023,\n",
       " 0.0,\n",
       " 0.7351,\n",
       " 0.3182,\n",
       " 0.4588,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3182,\n",
       " 0.3182,\n",
       " -0.2732,\n",
       " 0.3182,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2023,\n",
       " 0.4588,\n",
       " 0.872,\n",
       " -0.5267,\n",
       " 0.0,\n",
       " -0.128,\n",
       " 0.0,\n",
       " -0.2732,\n",
       " 0.4939,\n",
       " 0.8074,\n",
       " 0.4939,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.34,\n",
       " 0.4404,\n",
       " 0.4404,\n",
       " 0.6486,\n",
       " 0.8271,\n",
       " 0.4767,\n",
       " 0.0,\n",
       " 0.6808,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.765,\n",
       " 0.0,\n",
       " 0.4215,\n",
       " -0.2732,\n",
       " 0.4404,\n",
       " 0.5106,\n",
       " 0.7845,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5574,\n",
       " 0.5719,\n",
       " 0.0,\n",
       " 0.6486,\n",
       " 0.9337,\n",
       " 0.0,\n",
       " 0.6124,\n",
       " 0.0,\n",
       " -0.5994,\n",
       " -0.5994,\n",
       " -0.5994,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5994,\n",
       " 0.0,\n",
       " -0.3182,\n",
       " 0.0,\n",
       " -0.6759,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7003,\n",
       " 0.2732,\n",
       " 0.0,\n",
       " 0.3818,\n",
       " 0.5719,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.34,\n",
       " 0.0772,\n",
       " 0.3231,\n",
       " 0.0,\n",
       " 0.875,\n",
       " -0.4404,\n",
       " 0.6124,\n",
       " 0.5719,\n",
       " 0.4215,\n",
       " 0.7184,\n",
       " 0.0,\n",
       " 0.4404,\n",
       " -0.1779,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5267,\n",
       " 0.5267,\n",
       " 0.1779,\n",
       " -0.5106,\n",
       " -0.1027,\n",
       " 0.1027,\n",
       " 0.1779,\n",
       " 0.0,\n",
       " 0.959,\n",
       " 0.6705,\n",
       " -0.4767,\n",
       " 0.3612,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.7402,\n",
       " -0.1027,\n",
       " 0.296,\n",
       " -0.1027,\n",
       " 0.0,\n",
       " 0.8555,\n",
       " 0.7003,\n",
       " -0.4404,\n",
       " 0.0,\n",
       " 0.6369,\n",
       " 0.0,\n",
       " 0.6901,\n",
       " -0.7964,\n",
       " 0.4019,\n",
       " 0.296,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7269,\n",
       " 0.2716,\n",
       " 0.4404,\n",
       " -0.5994,\n",
       " -0.5574,\n",
       " 0.296,\n",
       " 0.0,\n",
       " 0.0516,\n",
       " 0.4019,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.128,\n",
       " 0.0,\n",
       " 0.3612,\n",
       " 0.4472,\n",
       " 0.5574,\n",
       " -0.1531,\n",
       " 0.7096,\n",
       " 0.9538,\n",
       " 0.891,\n",
       " 0.4019,\n",
       " 0.6705,\n",
       " 0.8402,\n",
       " 0.5859,\n",
       " 0.0,\n",
       " -0.4404,\n",
       " 0.0,\n",
       " 0.743,\n",
       " 0.0,\n",
       " 0.4939,\n",
       " 0.0,\n",
       " 0.2129,\n",
       " 0.0516,\n",
       " 0.6369,\n",
       " -0.4939,\n",
       " 0.0,\n",
       " -0.34,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9694,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9559,\n",
       " 0.2732,\n",
       " 0.5574,\n",
       " 0.3818,\n",
       " 0.7184,\n",
       " 0.3818,\n",
       " 0.8591,\n",
       " 0.6197,\n",
       " 0.975,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4019,\n",
       " 0.2263,\n",
       " 0.6597,\n",
       " 0.0,\n",
       " 0.5423,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " -0.5994,\n",
       " -0.5423,\n",
       " 0.7579,\n",
       " 0.4404,\n",
       " 0.0,\n",
       " 0.5423,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5106,\n",
       " 0.0,\n",
       " -0.5574,\n",
       " 0.1027,\n",
       " 0.4215,\n",
       " 0.0,\n",
       " 0.5859,\n",
       " 0.0,\n",
       " 0.0258,\n",
       " -0.7096,\n",
       " 0.0,\n",
       " 0.0258,\n",
       " -0.4767,\n",
       " 0.6486,\n",
       " 0.3612,\n",
       " -0.0258,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2023,\n",
       " 0.0,\n",
       " -0.2023,\n",
       " 0.4588,\n",
       " 0.0,\n",
       " -0.6908,\n",
       " -0.6908,\n",
       " 0.5574,\n",
       " 0.3818,\n",
       " 0.0,\n",
       " -0.9547,\n",
       " 0.5267,\n",
       " 0.6597,\n",
       " -0.4019,\n",
       " 0.0,\n",
       " 0.9001,\n",
       " 0.0,\n",
       " 0.8271,\n",
       " 0.8271,\n",
       " 0.8625,\n",
       " 0.6124,\n",
       " 0.6249,\n",
       " 0.2023,\n",
       " 0.4767,\n",
       " 0.0,\n",
       " 0.2732,\n",
       " 0.4939,\n",
       " 0.9022,\n",
       " 0.5574,\n",
       " -0.2263,\n",
       " 0.5267,\n",
       " -0.2263,\n",
       " 0.4404,\n",
       " 0.9468,\n",
       " 0.8957,\n",
       " 0.9509,\n",
       " -0.8126,\n",
       " -0.4019,\n",
       " -0.6908,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1531,\n",
       " 0.4404,\n",
       " 0.5719,\n",
       " 0.6249,\n",
       " 0.6808,\n",
       " 0.0,\n",
       " 0.7184,\n",
       " 0.0,\n",
       " 0.3612,\n",
       " 0.6124,\n",
       " 0.0772,\n",
       " 0.3818,\n",
       " 0.0258,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4215,\n",
       " 0.0,\n",
       " -0.3612,\n",
       " -0.4939,\n",
       " -0.4939,\n",
       " 0.765,\n",
       " 0.5267,\n",
       " 0.7269,\n",
       " 0.4692,\n",
       " -0.3818,\n",
       " 0.6486,\n",
       " 0.0,\n",
       " -0.4939,\n",
       " 0.25,\n",
       " 0.765,\n",
       " 0.0,\n",
       " 0.4767,\n",
       " -0.2732,\n",
       " 0.6597,\n",
       " 0.2732,\n",
       " 0.0,\n",
       " -0.4767,\n",
       " -0.0772,\n",
       " 0.4767,\n",
       " 0.8225,\n",
       " 0.0,\n",
       " 0.0258,\n",
       " 0.6705,\n",
       " 0.7964,\n",
       " 0.4019,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4588,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3612,\n",
       " 0.4767,\n",
       " 0.6908,\n",
       " 0.8807,\n",
       " 0.5574,\n",
       " 0.7003,\n",
       " 0.0,\n",
       " 0.5106,\n",
       " 0.0,\n",
       " -0.1531,\n",
       " 0.7845,\n",
       " 0.0,\n",
       " 0.7351,\n",
       " 0.0,\n",
       " -0.4238,\n",
       " 0.5106,\n",
       " 0.0,\n",
       " 0.5423,\n",
       " -0.3818,\n",
       " -0.296,\n",
       " 0.3612,\n",
       " 0.0,\n",
       " -0.4404,\n",
       " 0.507,\n",
       " 0.34,\n",
       " 0.7003,\n",
       " 0.0,\n",
       " 0.1531,\n",
       " 0.5574,\n",
       " -0.529,\n",
       " -0.7906,\n",
       " 0.0,\n",
       " 0.6361,\n",
       " 0.872,\n",
       " 0.6369,\n",
       " -0.25,\n",
       " 0.5574,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.6124,\n",
       " -0.3089,\n",
       " 0.0,\n",
       " 0.4588,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5574,\n",
       " 0.0,\n",
       " 0.34,\n",
       " 0.296,\n",
       " 0.8516,\n",
       " 0.6808,\n",
       " 0.7184,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8126,\n",
       " -0.0772,\n",
       " 0.6597,\n",
       " 0.2023,\n",
       " 0.0,\n",
       " -0.4767,\n",
       " 0.3612,\n",
       " 0.802,\n",
       " 0.5719,\n",
       " 0.0,\n",
       " -0.296,\n",
       " 0.0,\n",
       " 0.1027,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6124,\n",
       " 0.0,\n",
       " -0.128,\n",
       " 0.8126,\n",
       " 0.5267,\n",
       " 0.3612,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2023,\n",
       " 0.1531,\n",
       " -0.3818,\n",
       " 0.7096,\n",
       " 0.9623,\n",
       " 0.9081,\n",
       " 0.9432,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7506,\n",
       " 0.5994,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6249,\n",
       " 0.8519,\n",
       " 0.0,\n",
       " 0.1027,\n",
       " 0.6369,\n",
       " 0.7943,\n",
       " 0.7003,\n",
       " 0.0,\n",
       " 0.3818,\n",
       " 0.5106,\n",
       " 0.743,\n",
       " 0.7184,\n",
       " 0.6486,\n",
       " 0.2732,\n",
       " 0.0,\n",
       " -0.5423,\n",
       " 0.0,\n",
       " -0.4215,\n",
       " 0.0,\n",
       " 0.0258,\n",
       " 0.1779,\n",
       " 0.3612,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9516,\n",
       " 0.743,\n",
       " 0.2732,\n",
       " 0.6908,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1531,\n",
       " -0.3612,\n",
       " 0.0516,\n",
       " -0.5423,\n",
       " -0.296,\n",
       " 0.0,\n",
       " 0.3182,\n",
       " 0.0,\n",
       " -0.5106,\n",
       " 0.836,\n",
       " 0.743,\n",
       " 0.3182,\n",
       " 0.5719,\n",
       " 0.4588,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9201,\n",
       " 0.8331,\n",
       " 0.25,\n",
       " 0.6249,\n",
       " 0.6486,\n",
       " -0.296,\n",
       " 0.3612,\n",
       " 0.0,\n",
       " 0.4939,\n",
       " 0.3818,\n",
       " 0.0,\n",
       " 0.2874,\n",
       " 0.0,\n",
       " -0.6808,\n",
       " 0.7096,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.8442,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4404,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2023]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment scores of each sentence referencing Gold\n",
    "gold_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dbefff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.875,\n",
       " 0.6808,\n",
       " 0.9403,\n",
       " 0.4019,\n",
       " 0.4019,\n",
       " 0.4404,\n",
       " 0.4588,\n",
       " 0.1263,\n",
       " -0.2732,\n",
       " 0.0,\n",
       " 0.2732,\n",
       " 0.9081,\n",
       " 0.0,\n",
       " 0.6597,\n",
       " 0.7184,\n",
       " 0.0,\n",
       " 0.5859,\n",
       " 0.0,\n",
       " 0.4404,\n",
       " 0.4404,\n",
       " 0.0,\n",
       " 0.4767,\n",
       " -0.7579,\n",
       " 0.0,\n",
       " -0.4215,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5719,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9337,\n",
       " 0.743,\n",
       " 0.3182,\n",
       " 0.6124,\n",
       " 0.0,\n",
       " 0.0772,\n",
       " 0.765,\n",
       " 0.2732,\n",
       " 0.959,\n",
       " 0.296,\n",
       " 0.8074,\n",
       " 0.4019,\n",
       " 0.0,\n",
       " -0.4404,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4939,\n",
       " 0.9714,\n",
       " 0.2129,\n",
       " 0.7579,\n",
       " 0.0,\n",
       " 0.4767,\n",
       " 0.4767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0258,\n",
       " 0.0,\n",
       " -0.296,\n",
       " 0.0516,\n",
       " 0.0258,\n",
       " -0.4588,\n",
       " -0.9547,\n",
       " -0.4019,\n",
       " 0.0,\n",
       " -0.2263,\n",
       " 0.9468,\n",
       " 0.9246,\n",
       " 0.7003,\n",
       " 0.0772,\n",
       " 0.6249,\n",
       " 0.8008,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2732,\n",
       " 0.4939,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7506,\n",
       " 0.296,\n",
       " 0.0,\n",
       " -0.4588,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.6124,\n",
       " 0.0,\n",
       " 0.507,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.7906,\n",
       " 0.872,\n",
       " 0.34,\n",
       " 0.4215,\n",
       " 0.2263,\n",
       " 0.2023,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4019,\n",
       " 0.0,\n",
       " 0.8126,\n",
       " 0.9432,\n",
       " 0.7506,\n",
       " 0.0,\n",
       " 0.1779,\n",
       " 0.4215,\n",
       " -0.1531,\n",
       " 0.7906,\n",
       " 0.7717,\n",
       " 0.5719,\n",
       " 0.3818,\n",
       " 0.0,\n",
       " -0.5267,\n",
       " 0.2874]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment scores of each sentence referencing Silver\n",
    "silver_sentiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
